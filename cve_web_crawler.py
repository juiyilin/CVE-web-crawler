# -*- coding: utf-8 -*-
"""webcrawler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vpzwuWLIKRJY-jPpiA-x0Q09Yg3xSbBO
"""

import requests
from bs4 import BeautifulSoup as bs
import pandas as pd

# funcion


def webcrawler(word, k, v):
    r = requests.get(
        'https://snort.org/rule_docs?rules_query=%s&search_type=standard&submit_rule_search=' % word)
    r = r.text
    soup = bs(r, 'html.parser')
    if soup.find_all('a', 'categories') == []:
        k += 'no results\n'
        v += 'no results\n'

    else:
        for kk, vv in zip(soup.find_all('a', 'categories'), soup.find_all(class_='snort_search_item')):
            k = k+kk.text.strip()+'\n'
            v = v+vv.text+'\n'

    return k, v


# main
data = pd.read_excel('內對外.xlsx')
data['CVE'] = data['CVE'].fillna('')
cvelist = list(data['CVE'])

for i in range(len(cvelist)):
    if '\n' in cvelist[i]:
        cvelist[i] = cvelist[i].split()
    else:
        continue


keys = []
values = []
for i, cve in enumerate(cvelist):
    if cve == '...' or cve == '':
        keys.append(cve+'\n')
        values.append(cve+'\n')

    else:
        k = ''
        v = ''
        if type(cve) == list:
            for cvein in cve:
                if cvein == '...' or cvein == '':
                    k = k+cvein+'\n'
                    v = v+cvein+'\n'

                else:
                    k, v = webcrawler(cvein, k, v)

        else:
            k, v = webcrawler(cve, k, v)

        keys.append(k)
        values.append(v)

    print('finish index', i+1, cve)

print('finished', len(cvelist), 'search')
data['SID'] = pd.Series(keys)
data['思科Snort Rule'] = pd.Series(values)
data['SID'] = data['SID'].str.strip()
data['思科Snort Rule'] = data['思科Snort Rule'].str.strip()
data.to_excel('CVE.xlsx', index=False)
'''
from google.colab import files
files.download('CVE.xlsx')
'''
